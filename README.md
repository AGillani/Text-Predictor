Text Predictor/ Quote Completer using Deep Learning (Recurrent Neural Networks + LSTM(128 layers))

The attached files are:

1) pred6Legend.py
Python file defining the model architecture and loading it

2) Pred6RUN.py
Python file loading the model and adding the GUI

3) history_legend6.p
binary runtime file containing the stored model

4) keras_model_legend6.h5
Hierarchical Data Format file which saves the multi-dimensional arrays of weights for the trained model

5) Gemmel, David - [Drenai Saga 1] - Legend .txt
Txt file containing the 348 page epic fantasy novel Legend by David Gemmel, which is our dataset

The problem was to build a language model which can act as a text predictor. We were loosely inspired by the predictive text on Android keyboards which suggests likely words which the user might enter based on their previous inputs.
Our idea was to use Deep Learning and Recurrent Neural Networks to build this model. The sequential nature of RNNs makes them a suitable choice for Natural Language Processing applications. As the user starts typing the next most likely words are generated based on a character level probability model.
Traditional methods of text prediction which do not employ machine learning predict words based on the last inputted word. They do not keep track of the whole sentence and the sequence/ rhythm of the entered word so they are not as good as Deep Learning approaches.
The major features of our Text Predictor are:

 • It is a Text Completer. Due to the character level modeling we used, it can complete the word the user is entering, even if that word isn’t complete i.e. it suggests word endings for half words
    
  • It is a Text Predictor. The model learns patterns of words and sequences employed in the English language from a large corpus of text (discussed later). It then suggests 5 words which have the highest probability of being the next word for the user to choose from. The interesting part is that despite being a character level model, the network predicts complete words by combining probable characters until a space or other punctuation mark is predicted.
    
 • It is a Text Generator. After entering an initial input, you can choose the next word from the list of words generated by the model and then choose the next predicted word which is generated considering the model’s own previous prediction. This can be used to generate long sequences of text which is an extremely interesting problem in the field of Natural Language Processing
 
 
 • Quote Completion. As our model learnt from a fiction/ fantasy book, it performs better on adding its own ending to quotes from fiction and completing song lyrics. This results in very interesting results as we demonstrated by getting the model to complete quotes from philosophers to famous quotes from High Fantasy to song lyrics. The results are equal part amusing and believable.
  
  
  • It has a user friendly Graphical Interface to assist entering the words and choosing the correct predictions. The predictions are then incorporated as part of the input sequence and the buttons are updated with the next likely words.
